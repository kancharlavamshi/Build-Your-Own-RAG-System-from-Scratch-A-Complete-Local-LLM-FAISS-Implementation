{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41769e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i haven't used chatgpt, i my self did this to learn new concepts, bcz if we keep on writing prompts you will loose your ability to think\n",
    "# No thinking no new ideas or concepts\n",
    "# use AI only when needed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03690084",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pypdf sentence-transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d861a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "77a36e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pypdf import PdfReader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first step is to extract the information, Here iam extracting text from pdf\n",
    "# \n",
    "def ext_text(path):\n",
    "    data = PdfReader(path)\n",
    "    df=\"\"\n",
    "    for x in data.pages:\n",
    "        x_text=x.extract_text()\n",
    "        if x_text:\n",
    "            df += x_text +\"\\n\"\n",
    "\n",
    "    return df        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74cacde",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/data/LM/rag_project/notes.pdf\"\n",
    "data = ext_text(path)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f61a847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will chunk the text with over lap\n",
    "def chunk_text(text, chunk_size=500,overlap=100):\n",
    "    chunks=[]\n",
    "    start=0 \n",
    "    while start < len(text):\n",
    "        end =  start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size-overlap\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df27c54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks=chunk_text(data)\n",
    "print(len(chunks),chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e01edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 571.38it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "## Load Embedding Model\n",
    "## emb =  SentenceTransformer(\"BAAI/bge-large-en-v1.5\",device=\"cuda:6\")\n",
    "emb = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cuda:6\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "922d2ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 24/24 [00:00<00:00, 59.46it/s]\n"
     ]
    }
   ],
   "source": [
    "# Here we will create embeddings using the above model\n",
    "embeddings = emb.encode(chunks,batch_size=10,convert_to_numpy=True,\n",
    "show_progress_bar=True)\n",
    "# here we convert each chunk into vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "76b01948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00338129, -0.09022815,  0.08497572, ..., -0.02394508,\n",
       "        -0.08998082,  0.04077885],\n",
       "       [-0.02748079, -0.02066522,  0.01795669, ..., -0.01886765,\n",
       "        -0.01819193,  0.01213914],\n",
       "       [ 0.0769441 , -0.06430613, -0.05270697, ...,  0.01947873,\n",
       "        -0.0101842 , -0.04567917],\n",
       "       ...,\n",
       "       [-0.08240927,  0.00592057,  0.03266907, ...,  0.00999525,\n",
       "         0.06170399, -0.02119865],\n",
       "       [ 0.00646808,  0.04383314, -0.00831901, ..., -0.03141473,\n",
       "         0.04569596, -0.02072829],\n",
       "       [ 0.05067067, -0.03856865,  0.06469955, ..., -0.05472222,\n",
       "        -0.05902956, -0.03183942]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2545b016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384,)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here each chunk is 384 number, each chunk is one point in High dimensional space\n",
    "# Faiss stores those points so we can ask later\n",
    "embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbd2d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------- Build FAISS Index --------####\n",
    "# here we store the emeddings (vectors) in high dimensional space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c7a70058",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = embeddings.shape[1]\n",
    "# we create faiss index (nocompression with L2 distance)\n",
    "index=faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8844d266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(231, 384)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index, \"/data/LM/rag_project/faiss_index.bin\")\n",
    "with open(\"/data/LM/rag_project/chunks.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunks, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9518e861",
   "metadata": {},
   "source": [
    "Question Answering (RAG + LLM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032b73a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.read_index(\"/data/LM/rag_project/faiss_index.bin\")\n",
    "\n",
    "with open(\"/data/LM/rag_project/chunks.pkl\", \"rb\") as f:\n",
    "    chunks = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d396bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 201/201 [00:00<00:00, 305.81it/s, Materializing param=model.norm.weight]                              \n"
     ]
    }
   ],
   "source": [
    "## Load Small LLM\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model = model.to(\"cuda:6\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1898046a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-21): 22 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "45bd363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrieval Function\n",
    "def retrieve(q,k=3):\n",
    "    q_emb=emb.encode([q],convert_to_numpy=True)\n",
    "    D,I=index.search(q_emb,k)\n",
    "    return [chunks[i] for i in I[0]]\n",
    "\n",
    "##Question → embedding\n",
    "\n",
    "##FAISS finds nearest chunks\n",
    "\n",
    "##Returns top-k most relevant text pieces\n",
    "\n",
    "##This is pure RAG retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e036003f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate Answer Function\n",
    "\n",
    "def generate_answer(q):\n",
    "    retrieve_chunk=retrieve(q)\n",
    "    context=\"\\n\\n\".join(retrieve_chunk)\n",
    "    prompt=f\"\"\" You are a helpful Machine learning assistant.\n",
    "    Answer ONLY using the provided CONTEXT.\n",
    "    If answer not found, say \"NOt found in Notes\"\n",
    "\n",
    "    Context:{context}\n",
    "    Question:{q}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:6\")\n",
    "\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=300,\n",
    "        temperature=0.3,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "    # Only decode newly generated tokens\n",
    "    generated_tokens = output[0][inputs[\"input_ids\"].shape[1]:]\n",
    "\n",
    "    answer = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "    return answer.strip()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ba063cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      "\n",
      "1. A machine learning system is a system that can learn from data without being explicitly programmed.\n",
      "    2. Machine learning is a field of computer science that deals with the design, implementation, and application of algorithms that can learn from data.\n",
      "    3. Machine learning systems can be used for a variety of tasks, including image recognition, speech recognition, natural language processing, and recommendation systems.\n",
      "    4. Machine learning is often used in industries such as finance, healthcare, and marketing, where it can help improve decision-making processes and provide more accurate predictions.\n",
      "    5. Machine learning is a rapidly growing field with many new techniques and approaches being developed every year.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    question = input(\"\\nAsk: \")\n",
    "\n",
    "    if question.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "\n",
    "    answer = generate_answer(question)\n",
    "    print(\"\\nAnswer:\\n\")\n",
    "    print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c4f012",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS Is what we build in this experiment\n",
    "\n",
    "# PDF → chunks → embeddings → FAISS\n",
    "#                          ↓\n",
    "# User Question → embed → search\n",
    "#                          ↓\n",
    "# Context + Question → LLM\n",
    "#                          ↓\n",
    "# Answer\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neelam_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
